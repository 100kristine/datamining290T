name: inverse
layout: true
class: left, top, inverse

---

# Linear Regression

---

# Types of Models animate:

  + Classifiers
  + Regressions
  + Clustering
  + Outlier

???

## Details

  + Classifiers: describes and distinguishes cases. Yelp may want to find a
    category for a business based on the reviews and business description
  + Regressions: Predict a continuous value. Eg. predict a home's selling
    price given sq footage, # of bedrooms
  + Clustering: find "natural" groups of data *without labels*
  + Outlier: find anomalous transactions, eg. finding fraud for credit cards

---

# Case Study

  + Housing prices: square footage
  <img src="img/housing-regression.gif"/>

???

## Problem

  + We'd like to know how to price a house based on the square footage
  + Let's pretend this is the data we have
  + How would we guess that value for 2500 sq ft?

---

# Solution? animate:

  + Find a line that represents the data
  + ```y = m*x + b```
  + A line that is not very far from the points

???

## Prompts

  + In English, how would you solve this?
  + How to mathematically represent the line?
  + What is a good line?

---

# Similarity

  + Main challenges in data mining: defining a specific metric for an intuition
  + Define distance for an individual point
  + Define how to aggregate distances together

???

## Challenge

  + This is big problem for engineering and math (stats) in general
  + We'll cover some concepts, but if you're ever stuck, try looking in related
    fields
  + What are some of the ways we can measure distance between points?
    Euclidian, Manhattan, Euclidian == L_2 norm
  + What is a way to aggrgate numbers? sum, sum of squares, sum of logs
  + Differences between the last two?

---

## Log & Square two_col:

  + Log: Useful for de-emphasizing large raw differences
  + Square: Useful for taking the approximate absolute value
  <img src="img/logx.gif"/>

---

# Point Distance two_col:

  + =y= distance from line
  + Intuitively: error in estimate
  + ```h(x) = m*x + b```
  + ```err = h(x) - y```
  <img src="img/error.gif"/>

???

## Error

  + We want the difference from what we estimate to be the value to what the
    value actually is

---

# Aggregate animate:

  + ```sum```
  + What about negative error?
  + Sum of squares
  + ```err = sum( (h(x) - y)**2 for x,y in dataset) / len(dataset)```

???

## Questions

  + Now we have info about all the errors from points, how to summarize?
  + Some points have negative error, some positive? Do they cancel each other
    out?
  + Imagine data set of two points: one solutions covers lines, other divides
    them. Which is better?
  + Use our squaring trick to make sure we don't have any negative values
  + Normalize by the number of points

---

# Fitness Function

  + Measures the quality or cost of the solution
  + *Key* ingredient for data mining algorithms
  + If you can measure it, you can find the best solution

???

## Fitness

  + Function spits out a metric. Metric can be thought of as *fitness* or
    *cost*
  + Find the maximum or minimum of that metric
  + Depending on your fitness function, this can be easy or difficult
  + img: http://onlinestatbook.com

---

# Understanding Error

  <img src="img/Linear_regression.svg.png"/>
  Several possible solutions

???

## Error

  + What happens to the error as we move line around?
  + Decreases until best fit, then increases
  + What happens if we plot this error? Say, slope (x) against error (y)?

---

# Solution as Minimization two_col:

  + Error is a parabola
  + Several methods for finding the minimum
  + Two categories: analytical, approximations
<img src="img/parabola.png"/>

---

# Solution Approximation

  + Some fitness functions can be difficult to solve analytically
  + Alternative: iteratively get closer to the solution
  + Stop when answer is close enough

???

## Analytical

  + How to find the minimum of functions in general?
  + Take derivative, find 0
  + Taking derivative can be complex or impossible (discontinuities) for some
    functions, or solving for 0 is difficult
  + Instead, well keep getting closer to the minimum using the function we
    already have

---

# Gradient Descent two_col:

  1. Estimate current gradient (derivative)
  1. Take a step (=a * deriv=) in the direction of the gradient
  1. Step size is small, stop. Else repeat.
  <img src="img/parabola.png"/>

???

## Steps

  + Take gradient by looking at the local derivative, or perturbating x
  + Choose ```a= as step size weight: big =a``` is large step size
  + If ```deriv``` is large, will also make you step size large.
  + If ```deriv``` is large, probably means you are far away from minimum
  + Keep repeating
  + What happens if =a= is too small?
  + What happens if =a= is too big?

---

# General Case two_col:

  + Formulate fitness function for your problem
  + Use analytics or approximations to find min/max
  + Approximations: Newton's Method, Gradient Descent
  <img src="img/error-reduce.png"/>

???

## Approximate visualization

  + Desired output of the error as gradient descent runs
  + maybe some local problems, as step size is too big, but slowly move down to
    a small amount of error

---

# Support Vector Machines

---

# Decision Trees two_col:

  + Great for separable attributes
  + Rules operate on independent attributes
  + Classes separable along an axis/attribute
  <img src="img/tree.png"/>

---

## Linearly Separable

  + How to handle case where separator line is not along an axis?
  <img src="img/dataset_linsep.png"/>

???

## Details

  + Could say if ```x>2``` and ```y>2```, but not a great intuitive fit
  + Draw a line that takes both into account
  + ```y = m*x + b```
  + img: http://www.eric-kim.net/eric-kim-net/posts/1/kernel_trick.html

---

# Possibilities

  + Many lines *could* separate these classes
  <img src="img/dataset_linsep.png"/>

???

## Best?

  + Which is the best?
  + Why?

---

## Best Separator two_col:

  + Best line gives the most distance between the two classes
  + Measure distance between closest points
  + Closest points == support vectors
  <img src="img/separable.jpg"/>

???

## Points, Vectors

  + Points can be represented as vectors
  + Vector math can be easier to express succinctly
  + img: http://www.sciencedirect.com/science/article/pii/S1072751511001918

---

# Dimensions

  + When separating two dimensions, we need a line
  + When separating 3 dimensions?
  + 4 dimensions?

???

## Vocabulary

  + Plane
  + Hyperplane

---

# Expressing the Hyperplane animate:

  + ```y = m*x + b```
  + ```x_2 = m*x_1 + b```
  + ```0 = m*x_1 + b - x_2```
  + ```0 = [m -1] * [x_1, x_2] + b```
  + ```0 = w * x + b```

???

## Questions

  + How do you mathematically represent a line?
  + Now, we're not going to think of a new letter for every dimension, we're
    just going to say x_1 , x_2 , x_3 ...
  + Rewrite mathematically
  + How to add more dimensions? x_22? Express x as a vector of all attributes
  + Again, don't want to come up with a bunch more letters after =m=, so use
    ```w= as the matrix representing all the =m``` slopes

---

# Challenge two_col:

  + Find ```w=, =b``` such that ```w * x + b``` maximizes the distance between the
    support vectors
  <img src="img/svm.png"/>

---

# Maximizing Fitness Function two_col:

  + Now we have a fitness function and parameters we're trying to optimize
  + Sound familiar?
  <img src="img/svm.png"/>

---

# Kernel Tricks two_col:

  + SVM good for linearly separable data
  + How to handle other data?
  <img src="img/svm-circular.jpg"/>

---

## Polynomial Kernel

  + Transform it into linearly separable
  + What function can we apply to these data points to make them separable?
  <img src="img/svm-circular.jpg"/>

???

### Square

  + Square all of them

---

## Polynomial Kernel

  <img src="img/kernel-trick.jpg"/>

  Now apply SVM

???

## Details

  + img: http://www.sciencedirect.com/science/article/pii/S1072751511001918

---

# *Break*




---

Slide 1
  main
next slide is animated
Slide 2
  main
  notes
    a series of sections will work better for some definitions
    a series of sections will work better for some definitions
    a series of sections will work better for some definitions
    a series of sections will work better for some definitions
Slide 3
  main
    does image fit? is it OK on a dark background?
  notes
next slide is animated
Slide 4
  main
  notes
Slide 5
  main
  notes
next slide is two column
Slide 6
  main
    a series of sections will work better for some definitions
    a series of sections will work better for some definitions
    does image fit? is it OK on a dark background?
next slide is two column
Slide 7
  main
    does image fit? is it OK on a dark background?
  notes
next slide is animated
Slide 8
  main
  notes
Slide 9
  main
  notes
Slide 10
  main
    does image fit? is it OK on a dark background?
  notes
next slide is two column
Slide 11
  main
    does image fit? is it OK on a dark background?
Slide 12
  main
  notes
next slide is two column
Slide 13
  main
    does image fit? is it OK on a dark background?
  notes
next slide is two column
Slide 14
  main
    does image fit? is it OK on a dark background?
  notes
Slide 15
  main
next slide is two column
Slide 16
  main
    does image fit? is it OK on a dark background?
Slide 17
  main
    does image fit? is it OK on a dark background?
  notes
Slide 18
  main
    does image fit? is it OK on a dark background?
  notes
next slide is two column
Slide 19
  main
    does image fit? is it OK on a dark background?
  notes
Slide 20
  main
  notes
next slide is animated
Slide 21
  main
  notes
next slide is two column
Slide 22
  main
    does image fit? is it OK on a dark background?
next slide is two column
Slide 23
  main
    does image fit? is it OK on a dark background?
next slide is two column
Slide 24
  main
    does image fit? is it OK on a dark background?
Slide 25
  main
    does image fit? is it OK on a dark background?
  notes
Slide 26
  main
    does image fit? is it OK on a dark background?
  notes
Slide 27
  main
Headings are the right level?
